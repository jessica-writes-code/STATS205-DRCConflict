source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
rm(list=ls())
# Set input path
input <- "/Users/jmoore523/Dropbox/Graduate School/Q3 - Spring 2016/STATS205/Assignment 6/Input"
# Import libraries
library(datasets)
library(waveslim)
library(igraph)
# Load data
data(sunspots)
# Obtain first 512 observations
sun <- sunspots[1:512]
# Obtain wavelet coefficients
sun.wav <- dwt(sun, n.levels=9)
# Create vector of coefficients
sun.coef <- unlist(sun.wav)
# Make histogram of untransformed data
hist(sun, breaks=20, main="Original Data", xlab="Values")
dev.off
dev.off()
rm(list=ls())
# Set input path
input <- "/Users/jmoore523/Dropbox/Graduate School/Q3 - Spring 2016/STATS205/Assignment 6/Input"
# Import libraries
library(datasets)
library(waveslim)
library(igraph)
# Load data
data(sunspots)
# Obtain first 512 observations
sun <- sunspots[1:512]
# Obtain wavelet coefficients
sun.wav <- dwt(sun, n.levels=9)
# Create vector of coefficients
sun.coef <- unlist(sun.wav)
# Make histogram of untransformed data
hist(sun, breaks=20, main="Original Data", xlab="Values")
# Make histogram of wavelet coefficients
hist(sun.coef, breaks=20, main="Wavelet Coefficients", xlab="Coefficients")
# Make histogram of highest level of detail coefficients
hist(sun.wav[[1]], breaks=20, main="Coefficients: Highest Level of Detail", xlab="Coefficient Values")
# Make ECDF of highest level of detail coefficients
plot(ecdf(sun.wav[[1]]), main="ECDF of Coefficients: Highest Level of Detail", xlab="Coefficient Values", ylab="Cumulative Proportion of Data")
# Plot Original Data
y <- idwt(sun.wav)
plot.ts(y, main="Original Data", ylab="Sunspots")
# Plot Original Data
y <- idwt(sun.wav)
plot.ts(y, main="Original Data", ylab="Sunspots")
# Threshold Coefficients using SureShrink & VisuShrink
## SureShrink
for (i in c(1,4,9)) {
for (j in c(TRUE,FALSE)) {
### Temporary 'dwt' object
sun.wav.temp <- sun.wav
### Apply SureShrink
sun.sureshrink <- sure.thresh(sun.wav, max.level = i, hard = j)
for (k in 1:9) {
sun.wav.temp[[k]] <- sun.sureshrink[[k]]
}
### Plot Reconstruction
y <- idwt(sun.wav.temp)
thresh.type <- ifelse(j == TRUE, "Hard", "Soft")
plot.ts(y, main=paste0("SureShrink: Max Level = ",i,", ", thresh.type," Threshold"), ylab="Sunspots")
}
}
## VisuShrink
for (i in c(1,4,9)) {
for (j in c(TRUE,FALSE)) {
### Temporary 'dwt' object
sun.wav.temp <- sun.wav
### Apply SureShrink
sun.visushrink <- universal.thresh(sun.wav, max.level = i, hard = j)
for (k in 1:9) {
sun.wav.temp[[k]] <- sun.visushrink[[k]]
}
### Plot Reconstruction
y <- idwt(sun.wav.temp)
thresh.type <- ifelse(j == TRUE, "Hard", "Soft")
plot.ts(y, main=paste0("VisuShrink: Max Level = ",i,", ", thresh.type," Threshold"), ylab="Sunspots")
}
}
# Load data
fb <- read.table(paste0(input,"/facebook_combined.txt"), stringsAsFactors = FALSE)
View(fb)
names(fb) <- c("firstnode","secondnode")
# Add 1 to each node value, because
# Facebook Indexes Starting at 0
# and R Indexes Starting at 1
fb$firstnode <- fb$firstnode + 1
fb$secondnode <- fb$secondnode + 1
View(fb)
# Create adjacency matrix
## Set up empty matrix
nodes <- length(unique(fb$firstnode))
fb.adj <- matrix(rep(0,nodes^2), nrow=nodes, ncol=nodes)
rownames(fb.adj) <- seq(1,nodes)
colnames(fb.adj) <- seq(1,nodes)
View(fb)
View(fb.adj)
## Fill with edge indicators
for (k in 1:nodes) {
i <- fb[k,]$firstnode
j <- fb[k,]$secondnode
fb.adj[i,j] <- 1
fb.adj[j,i] <- 1
}
View(fb.adj)
# Limit to Top 500 Nodes, by Empirical Degree
## Count empirical degree
fb.empdeg <- apply(fb.adj, 1, sum)
fb.empdeg.order <- names(sort(fb.empdeg, decreasing=TRUE))
## Subset adjacency matrix
n <- 500
fb.empdeg
sort(fb.empdeg, decreasing=TRUE)
fb.empdeg.order <- names(sort(fb.empdeg, decreasing=TRUE))
## Subset adjacency matrix
n <- 500
fb.adj <- fb.adj[,fb.empdeg.order]
fb.adj <- fb.adj[fb.empdeg.order,]
View(fb.adj)
View(fb.adj)
fb.adj <- fb.adj[1:n, 1:n]
# Save original matrix
fb.adj.orig <- fb.adj
a <- min(fb.adj)
b <- max(fb.adj)
fb.adj <- fb.adj - (a+b)/2
fb.adj <- fb.adj / ((b-a)/2)
View(fb.adj)
View(fb.adj)
# Take SVD of matrix (Lecture Step 3)
adj.svd <- svd(fb.adj)
U <- adj.svd$u
V <- adj.svd$v
s <- adj.svd$d
eta <- 0.01
t <- (2 + eta)*sqrt(n*phat)
phat <- 1
t <- (2 + eta)*sqrt(n*phat)
s[s < t] <- 0
W <- U %*% diag(s) %*% t(V)
W <- 1/phat * W
col.names <- colnames(fb.adj)
row.names <- rownames(fb.adj)
colnames(W) <- col.names
rownames(W) <- row.names
W[W > 1] <- 1
W[W < -1] <- -1
View(W)
# Map Values to [0,1] (Lecture Step 8)
W <- W/2
W <- W + .5
fb.empdeg.sub <- apply(fb.adj.orig, 1, sum)
fb.empdeg.order.sub <- names(sort(fb.empdeg.sub, decreasing=TRUE))
fb.empdeg.order.sub
fb.empdeg.sub
fb.empdeg.order.sub <- names(sort(fb.empdeg.sub, decreasing=TRUE))
fb.adj.orig <- fb.adj.orig[,fb.empdeg.order.sub]
fb.adj.orig <- fb.adj.orig[fb.empdeg.order.sub,]
View(fb.adj.orig)
heatmap(fb.adj.orig, col=grey(c(0.9,0)), Rowv = NA, Colv = NA, labRow = NA,
labCol = NA, symm=TRUE, margins=c(0,0))
rm(list=ls())
input <- "/Users/jmoore523/Dropbox/Graduate School/Q3 - Spring 2016/STATS315B/Assignment 3/Individual Work/Input"
# Set seed
set.seed(315)
# Load libraries
library(nnet)
# Load Training & Test Set
spam.train <- read.csv(paste0(input,"/Spam_Train.txt"), header=FALSE)
spam.test <- read.csv(paste0(input,"/Spam.Test.txt"), header=FALSE)
# Add Variable Names
spam.names <- c("word_freq_make","word_freq_address","word_freq_all","word_freq_3d",
"word_freq_our","word_freq_over","word_freq_remove","word_freq_internet",
"word_freq_order","word_freq_mail","word_freq_receive","word_freq_will",
"word_freq_people","word_freq_report","word_freq_addresses",
"word_freq_free","word_freq_business","word_freq_email","word_freq_you",
"word_freq_credit","word_freq_your","word_freq_font","word_freq_000",
"word_freq_money","word_freq_hp","word_freq_hpl","word_freq_george",
"word_freq_650","word_freq_lab","word_freq_labs","word_freq_telnet",
"word_freq_857","word_freq_data","word_freq_415","word_freq_85",
"word_freq_technology","word_freq_1999","word_freq_parts","word_freq_pm",
"word_freq_direct","word_freq_cs","word_freq_meeting",
"word_freq_original","word_freq_project","word_freq_re","word_freq_edu",
"word_freq_table","word_freq_conference","char_freq_;","char_freq_(",
"char_freq_[","char_freq_!","char_freq_$","char_freq_#",
"capital_run_length_average","capital_run_length_longest",
"capital_run_length_total","emailtype")
names(spam.train) <- spam.names
names(spam.test) <- spam.names
mean(spam.train$word_freq_make)
mean(spam.train[,1])
# Standardize Variables
## Vector to record means & SDs from training data
means <- rep(NA,57)
sds <- rep(NA,57)
## Loop through training data variables,
## recording means & SDs
for (i in 1:57) {
means[i] <- mean(spam.train[,i])
sds <- sd(spam.train[,i])
}
## Loop through training & test data variables,
## standardizing based on means & SDs recorded previously
for (i in 1:57) {
### Training Data: Subtract Mean
spam.train[,i] <- spam.train[,i] - mean[i]
### Test Data: Subtract Mean
spam.test[,i] <- spam.test[,i] - mean[i]
### Training Data: Divided by SD
spam.train[,i] <- spam.train[,i]/sds[i]
### Test Data: Divided by SD
spam.test[,i] <- spam.test[,i]/sds[i]
}
rm(list=ls())
input <- "/Users/jmoore523/Dropbox/Graduate School/Q3 - Spring 2016/STATS315B/Assignment 3/Individual Work/Input"
# Set seed
set.seed(315)
# Load libraries
library(nnet)
# Load Training & Test Set
spam.train <- read.csv(paste0(input,"/Spam_Train.txt"), header=FALSE)
spam.test <- read.csv(paste0(input,"/Spam.Test.txt"), header=FALSE)
# Add Variable Names
spam.names <- c("word_freq_make","word_freq_address","word_freq_all","word_freq_3d",
"word_freq_our","word_freq_over","word_freq_remove","word_freq_internet",
"word_freq_order","word_freq_mail","word_freq_receive","word_freq_will",
"word_freq_people","word_freq_report","word_freq_addresses",
"word_freq_free","word_freq_business","word_freq_email","word_freq_you",
"word_freq_credit","word_freq_your","word_freq_font","word_freq_000",
"word_freq_money","word_freq_hp","word_freq_hpl","word_freq_george",
"word_freq_650","word_freq_lab","word_freq_labs","word_freq_telnet",
"word_freq_857","word_freq_data","word_freq_415","word_freq_85",
"word_freq_technology","word_freq_1999","word_freq_parts","word_freq_pm",
"word_freq_direct","word_freq_cs","word_freq_meeting",
"word_freq_original","word_freq_project","word_freq_re","word_freq_edu",
"word_freq_table","word_freq_conference","char_freq_;","char_freq_(",
"char_freq_[","char_freq_!","char_freq_$","char_freq_#",
"capital_run_length_average","capital_run_length_longest",
"capital_run_length_total","emailtype")
names(spam.train) <- spam.names
names(spam.test) <- spam.names
# Standardize Variables
## Vector to record means & SDs from training data
means <- rep(NA,57)
sds <- rep(NA,57)
## Loop through training data variables,
## recording means & SDs
for (i in 1:57) {
mu[i] <- mean(spam.train[,i])
sds[i] <- sd(spam.train[,i])
}
## Loop through training & test data variables,
## standardizing based on means & SDs recorded previously
for (i in 1:57) {
### Training Data: Subtract Mean
spam.train[,i] <- spam.train[,i] - means[i]
### Test Data: Subtract Mean
spam.test[,i] <- spam.test[,i] - means[i]
### Training Data: Divided by SD
spam.train[,i] <- spam.train[,i]/sds[i]
### Test Data: Divided by SD
spam.test[,i] <- spam.test[,i]/sds[i]
}
View(spam.test)
# Load Training & Test Set
spam.train <- read.csv(paste0(input,"/Spam_Train.txt"), header=FALSE)
spam.test <- read.csv(paste0(input,"/Spam.Test.txt"), header=FALSE)
# Add Variable Names
spam.names <- c("word_freq_make","word_freq_address","word_freq_all","word_freq_3d",
"word_freq_our","word_freq_over","word_freq_remove","word_freq_internet",
"word_freq_order","word_freq_mail","word_freq_receive","word_freq_will",
"word_freq_people","word_freq_report","word_freq_addresses",
"word_freq_free","word_freq_business","word_freq_email","word_freq_you",
"word_freq_credit","word_freq_your","word_freq_font","word_freq_000",
"word_freq_money","word_freq_hp","word_freq_hpl","word_freq_george",
"word_freq_650","word_freq_lab","word_freq_labs","word_freq_telnet",
"word_freq_857","word_freq_data","word_freq_415","word_freq_85",
"word_freq_technology","word_freq_1999","word_freq_parts","word_freq_pm",
"word_freq_direct","word_freq_cs","word_freq_meeting",
"word_freq_original","word_freq_project","word_freq_re","word_freq_edu",
"word_freq_table","word_freq_conference","char_freq_;","char_freq_(",
"char_freq_[","char_freq_!","char_freq_$","char_freq_#",
"capital_run_length_average","capital_run_length_longest",
"capital_run_length_total","emailtype")
names(spam.train) <- spam.names
names(spam.test) <- spam.names
rm(list=ls())
input <- "/Users/jmoore523/Dropbox/Graduate School/Q3 - Spring 2016/STATS315B/Assignment 3/Individual Work/Input"
# Set seed
set.seed(315)
# Load libraries
library(nnet)
# Load Training & Test Set
spam.train <- read.csv(paste0(input,"/Spam_Train.txt"), header=FALSE)
spam.test <- read.csv(paste0(input,"/Spam.Test.txt"), header=FALSE)
# Add Variable Names
spam.names <- c("word_freq_make","word_freq_address","word_freq_all","word_freq_3d",
"word_freq_our","word_freq_over","word_freq_remove","word_freq_internet",
"word_freq_order","word_freq_mail","word_freq_receive","word_freq_will",
"word_freq_people","word_freq_report","word_freq_addresses",
"word_freq_free","word_freq_business","word_freq_email","word_freq_you",
"word_freq_credit","word_freq_your","word_freq_font","word_freq_000",
"word_freq_money","word_freq_hp","word_freq_hpl","word_freq_george",
"word_freq_650","word_freq_lab","word_freq_labs","word_freq_telnet",
"word_freq_857","word_freq_data","word_freq_415","word_freq_85",
"word_freq_technology","word_freq_1999","word_freq_parts","word_freq_pm",
"word_freq_direct","word_freq_cs","word_freq_meeting",
"word_freq_original","word_freq_project","word_freq_re","word_freq_edu",
"word_freq_table","word_freq_conference","char_freq_;","char_freq_(",
"char_freq_[","char_freq_!","char_freq_$","char_freq_#",
"capital_run_length_average","capital_run_length_longest",
"capital_run_length_total","emailtype")
names(spam.train) <- spam.names
names(spam.test) <- spam.names
# Standardize Variables
## Vector to record means & SDs from training data
mus <- rep(NA,57)
sds <- rep(NA,57)
## Loop through training data variables,
## recording means & SDs
for (i in 1:57) {
mus[i] <- mean(spam.train[,i])
sds[i] <- sd(spam.train[,i])
}
## Loop through training & test data variables,
## standardizing based on means & SDs recorded previously
for (i in 1:57) {
### Training Data: Subtract Mean
spam.train[,i] <- spam.train[,i] - mus[i]
### Test Data: Subtract Mean
spam.test[,i] <- spam.test[,i] - mus[i]
### Training Data: Divided by SD
spam.train[,i] <- spam.train[,i]/sds[i]
### Test Data: Divided by SD
spam.test[,i] <- spam.test[,i]/sds[i]
}
sd(spam.train[,2])
sd(spam.train[,3])
View(spam.train)
source('~/.active-rstudio-document', echo=TRUE)
View(temp1)
View(spam.train)
View(temp2)
View(spam.test)
# Function to fit one-hidden-layer NN with 'best.units' units
fit.nn <- function(d) {
## Set different sets starting values for predictors
num.wts <- 58*best.units + best.units + 1
wts <- runif(num.wts, min=-0.5, max=0.5)
## Fit NN
nnet.cur <- nnet(emailtype ~ ., data=spam.train1, size = best.units, Wts = wts, decay = d)
## Predict on 'Spam.Test'
pred.curr <- predict(nnet.cur, newdata=spam.test, type='raw')
return(pred.curr)
}
# Vector for misclassification errors
misclass <- rep(NA,11)
# Vector of weight decays
wtdec.all <- seq(0,1,length.out=11)
# Fit one-hidden-layer NN with 'best.units' units
# for 0, 0.1, ..., 1 values of weight decay
for (i in 1:11) {
wtdec <- wtdec.all[i]
## Create 10 runs with different starting values
sink("/dev/null")
pred1 <- fit.nn(wtdec)
pred2 <- fit.nn(wtdec)
pred3 <- fit.nn(wtdec)
pred4 <- fit.nn(wtdec)
pred5 <- fit.nn(wtdec)
pred6 <- fit.nn(wtdec)
pred7 <- fit.nn(wtdec)
pred8 <- fit.nn(wtdec)
pred9 <- fit.nn(wtdec)
pred10 <- fit.nn(wtdec)
sink()
## Average predictions
pred.all <- cbind(pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred8, pred9, pred10)
pred.final <- apply(pred.all, 1, mean)
## Create binary prediction from probabilities
pred.final.bin <- rep(0, length(pred.final))
pred.final.bin[pred.final > 0.5] <- 1
## Record misclassification error
misclass.table <- table(pred.final.bin, spam.test$emailtype)
misclass.rate <- (misclass.table[1,2] + misclass.table[2,1])/sum(misclass.table)
misclass[i] <- misclass.rate
}
# Find best weight decay
best.wtdec <- wtdec.all[which.min(misclass)]
# Misclassification rate for best weight decay
misclass[which.min(misclass)]
# Vector for misclassification errors
misclass <- rep(NA,10)
# Fit one-hidden-layer NNs with 1 through 10 units in the hidden layer
for (i in 1:10) {
## Set different sets starting values for predictors
num.wts <- 58*i + i + 1
wts <- runif(num.wts, min=-0.5, max=0.5)
## Fit NN
sink("/dev/null")
nnet.cur <- nnet(emailtype ~ ., data=spam.train, size = i, Wts = wts)
sink()
## Predict on Test Set 1
predictions <- predict(nnet.cur, newdata=spam.test, type='raw')
## Record misclassification error on test data
misclass.table <- table(predictions, spam.test$emailtype)
misclass.rate <- (misclass.table[1,2] + misclass.table[2,1])/sum(misclass.table)
misclass[i] <- misclass.rate
}
# Identify number of units w/ lowest misclass rate
best.units <- which.min(misclass)
best.units
source('~/.active-rstudio-document', echo=TRUE)
misclass
print(misclass)
